{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd68daf-a993-4b0e-8721-37d1b62eca2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,664</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                 │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ mu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ log_var (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ mu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], log_var[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m4\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │          \u001b[38;5;34m17,664\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                 │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │           \u001b[38;5;34m2,080\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ mu (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 │              \u001b[38;5;34m33\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ log_var (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 │              \u001b[38;5;34m33\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ out (\u001b[38;5;33mConcatenate\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │ mu[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], log_var[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,810</span> (77.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,810\u001b[0m (77.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,810</span> (77.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,810\u001b[0m (77.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "24/24 - 4s - 187ms/step - loss: 1.0872 - val_loss: 1.1818\n",
      "Epoch 2/15\n",
      "24/24 - 0s - 19ms/step - loss: 0.7674 - val_loss: 1.0833\n",
      "Epoch 3/15\n",
      "24/24 - 0s - 20ms/step - loss: 0.7199 - val_loss: 1.0814\n",
      "Epoch 4/15\n",
      "24/24 - 0s - 19ms/step - loss: 0.7280 - val_loss: 1.0856\n",
      "Epoch 5/15\n",
      "24/24 - 0s - 19ms/step - loss: 0.7157 - val_loss: 1.0844\n",
      "Epoch 6/15\n",
      "24/24 - 0s - 19ms/step - loss: 0.6988 - val_loss: 1.0789\n",
      "Epoch 7/15\n",
      "24/24 - 0s - 19ms/step - loss: 0.6982 - val_loss: 1.0753\n",
      "Epoch 8/15\n",
      "24/24 - 0s - 19ms/step - loss: 0.6961 - val_loss: 1.0742\n",
      "Epoch 9/15\n",
      "24/24 - 0s - 19ms/step - loss: 0.6911 - val_loss: 1.0858\n",
      "Epoch 10/15\n",
      "24/24 - 0s - 19ms/step - loss: 0.6963 - val_loss: 1.0803\n",
      "Epoch 11/15\n",
      "24/24 - 0s - 19ms/step - loss: 0.6885 - val_loss: 1.0757\n",
      "Epoch 12/15\n",
      "24/24 - 0s - 19ms/step - loss: 0.6859 - val_loss: 1.0809\n",
      "Results metrics:\n",
      "  rmse: 1.764058\n",
      "  mae: 1.584527\n",
      "  coverage_80: 0.812500\n",
      "  mean_width_80: 4.258481\n",
      "  coverage_95: 0.987500\n",
      "  mean_width_95: 6.512785\n",
      "Artifacts saved in: ts_forecast_outputs\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "probabilistic_lstm_forecast.py\n",
    "\n",
    "Advanced time series forecasting with an LSTM producing probabilistic outputs (mean and log-variance).\n",
    "Includes:\n",
    " - synthetic daily dataset generation (trend + seasonality + heteroscedastic noise)\n",
    " - feature engineering (cyclic time features)\n",
    " - sequence generation for supervised learning\n",
    " - LSTM model that outputs mu and log_var and is trained with Gaussian NLL\n",
    " - Monte Carlo Dropout for epistemic uncertainty estimation\n",
    " - Evaluation: RMSE, MAE, coverage and mean interval width (80% and 95%)\n",
    " - Visualization of forecasts vs actuals with prediction intervals\n",
    "\n",
    "Author: ChatGPT (GPT-5 Thinking mini)\n",
    "Date: 2025-11\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# If running in an environment with TensorFlow:\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models, backend as K\n",
    "    tf.random.set_seed(SEED)\n",
    "    TF_AVAILABLE = True\n",
    "except Exception:\n",
    "    TF_AVAILABLE = False\n",
    "\n",
    "# -------------------------\n",
    "# Utility functions\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "def generate_synthetic_daily_series(\n",
    "    start: str = \"2018-01-01\",\n",
    "    days: int = 3 * 365,\n",
    "    trend_slope: float = 0.001,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate synthetic daily time series with trend, yearly and weekly seasonality,\n",
    "    and heteroscedastic noise.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    start : str\n",
    "        Start date for the series.\n",
    "    days : int\n",
    "        Number of days to generate.\n",
    "    trend_slope : float\n",
    "        Linear trend per day.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns ['ds', 'y'] where ds is datetime and y is value.\n",
    "    \"\"\"\n",
    "    rng = pd.date_range(start=start, periods=days, freq=\"D\")\n",
    "    t = np.arange(days).astype(float)\n",
    "\n",
    "    trend = trend_slope * t\n",
    "    yearly = 2.0 * np.sin(2 * np.pi * t / 365.25)\n",
    "    weekly = 0.5 * np.sin(2 * np.pi * t / 7.0)\n",
    "\n",
    "    base_noise_scale = 0.5 + 0.5 * (1 + np.sin(2 * np.pi * t / 365.25))\n",
    "    noise = np.random.normal(scale=base_noise_scale)\n",
    "    outliers = np.random.choice([0.0, 0.0, 0.0, 3.0], size=days, p=[0.7, 0.15, 0.14, 0.01])\n",
    "    y = 10 + trend + yearly + weekly + noise + outliers\n",
    "\n",
    "    df = pd.DataFrame({\"ds\": rng, \"y\": y})\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create time-based features (cyclic encodings) for the time series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame with 'ds' and 'y'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with additional features.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"day_of_year\"] = df[\"ds\"].dt.dayofyear\n",
    "    df[\"day_of_week\"] = df[\"ds\"].dt.dayofweek\n",
    "    df[\"month\"] = df[\"ds\"].dt.month\n",
    "    df[\"sin_doy\"] = np.sin(2 * np.pi * df[\"day_of_year\"] / 365.25)\n",
    "    df[\"cos_doy\"] = np.cos(2 * np.pi * df[\"day_of_year\"] / 365.25)\n",
    "    df[\"sin_dow\"] = np.sin(2 * np.pi * df[\"day_of_week\"] / 7.0)\n",
    "    df[\"cos_dow\"] = np.cos(2 * np.pi * df[\"day_of_week\"] / 7.0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_sequence_dataset(\n",
    "    df: pd.DataFrame,\n",
    "    feature_cols: List[str],\n",
    "    target_col: str,\n",
    "    lookback: int,\n",
    "    horizon: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create sequences (X) and targets (Y) for supervised learning.\n",
    "\n",
    "    Returns X shape (samples, lookback, features), Y shape (samples, horizon).\n",
    "    \"\"\"\n",
    "    values = df[feature_cols + [target_col]].values\n",
    "    n_features = len(feature_cols)\n",
    "    X, Y = [], []\n",
    "    for i in range(len(df) - lookback - horizon + 1):\n",
    "        x = values[i : i + lookback, :n_features]\n",
    "        y = values[i + lookback : i + lookback + horizon, -1]\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Probabilistic loss and model (TensorFlow implementation)\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "def gaussian_nll(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Gaussian negative log-likelihood loss. Expects y_pred to contain [mu, log_var].\n",
    "    \"\"\"\n",
    "    mu = y_pred[:, 0:1]\n",
    "    log_var = y_pred[:, 1:2]\n",
    "    precision = K.exp(-log_var)\n",
    "    return K.mean(0.5 * precision * K.square(y_true - mu) + 0.5 * log_var)\n",
    "\n",
    "\n",
    "def build_lstm_probabilistic(input_shape: Tuple[int, int], units: int = 64, dropout_rate: float = 0.2):\n",
    "    \"\"\"\n",
    "    Build an LSTM model that outputs mean and log-variance (aleatoric).\n",
    "    Dropout layers use `training=True` at call-time so MC Dropout is possible.\n",
    "    \"\"\"\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = layers.LSTM(units, return_sequences=False)(inp)\n",
    "    # Dropout with training=True in inference: set training argument when calling model\n",
    "    x = layers.Dropout(dropout_rate)(x, training=True)\n",
    "    x = layers.Dense(units // 2, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x, training=True)\n",
    "    mu = layers.Dense(1, name=\"mu\")(x)\n",
    "    log_var = layers.Dense(1, name=\"log_var\")(x)\n",
    "    out = layers.Concatenate(name=\"out\")([mu, log_var])\n",
    "    model = models.Model(inputs=inp, outputs=out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss=gaussian_nll)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mc_dropout_predict(model, X: np.ndarray, mc_samples: int = 100):\n",
    "    \"\"\"\n",
    "    Perform MC Dropout predictions to estimate predictive mean, epistemic var, and aleatoric var.\n",
    "\n",
    "    Returns:\n",
    "        mu_mean (n,), epistemic_var (n,), aleatoric_mean (n,)\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for _ in range(mc_samples):\n",
    "        pred = model(X, training=True)\n",
    "        preds.append(pred.numpy())\n",
    "    preds = np.stack(preds, axis=0)  # shape (mc, batch, 2)\n",
    "    mu_samples = preds[:, :, 0]\n",
    "    log_var_samples = preds[:, :, 1]\n",
    "    mu_mean = mu_samples.mean(axis=0)\n",
    "    epistemic_var = mu_samples.var(axis=0)\n",
    "    aleatoric_mean = np.exp(log_var_samples).mean(axis=0)\n",
    "    return mu_mean.ravel(), epistemic_var.ravel(), aleatoric_mean.ravel()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Evaluation helper\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "def evaluate_intervals(y_true, mu, total_var, ci_list=(0.8, 0.95)):\n",
    "    \"\"\"\n",
    "    Compute RMSE, MAE and coverage/mean interval width for given CIs.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, mu))\n",
    "    mae = mean_absolute_error(y_true, mu)\n",
    "    metrics[\"rmse\"] = float(rmse)\n",
    "    metrics[\"mae\"] = float(mae)\n",
    "    for ci in ci_list:\n",
    "        if np.isclose(ci, 0.8):\n",
    "            z = 1.281551565545\n",
    "        elif np.isclose(ci, 0.9):\n",
    "            z = 1.6448536269514722\n",
    "        elif np.isclose(ci, 0.95):\n",
    "            z = 1.959963984540054\n",
    "        else:\n",
    "            # fallback\n",
    "            from scipy.stats import norm  # scipy is common in ML envs\n",
    "            z = float(norm.ppf(0.5 + ci / 2.0))\n",
    "        half_width = z * np.sqrt(total_var)\n",
    "        lower = mu - half_width\n",
    "        upper = mu + half_width\n",
    "        coverage = float(np.mean((y_true >= lower) & (y_true <= upper)))\n",
    "        mean_width = float(np.mean(upper - lower))\n",
    "        metrics[f\"coverage_{int(ci*100)}\"] = coverage\n",
    "        metrics[f\"mean_width_{int(ci*100)}\"] = mean_width\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Main pipeline function\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "def run_pipeline(\n",
    "    days: int = 3 * 365,\n",
    "    lookback: int = 30,\n",
    "    horizon: int = 1,\n",
    "    train_ratio: float = 0.7,\n",
    "    val_ratio: float = 0.15,\n",
    "    epochs: int = 20,\n",
    "    batch_size: int = 32,\n",
    "    units: int = 64,\n",
    "    dropout_rate: float = 0.2,\n",
    "    mc_samples: int = 100,\n",
    "    artifact_dir: str = \"ts_forecast_outputs\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the full pipeline: data generation, preprocessing, training, prediction, evaluation, plotting.\n",
    "\n",
    "    Returns a dictionary of metrics and the artifact directory.\n",
    "    \"\"\"\n",
    "    if not TF_AVAILABLE:\n",
    "        raise RuntimeError(\"TensorFlow is not available in this environment. Install tensorflow and retry.\")\n",
    "\n",
    "    # 1) Data\n",
    "    df = generate_synthetic_daily_series(days=days)\n",
    "    df = create_features(df)\n",
    "    feature_cols = [\"sin_doy\", \"cos_doy\", \"sin_dow\", \"cos_dow\"]\n",
    "    X_all, Y_all = build_sequence_dataset(df, feature_cols, \"y\", lookback, horizon)\n",
    "    Y_all = Y_all.ravel()\n",
    "\n",
    "    n_samples = X_all.shape[0]\n",
    "    train_end = int(n_samples * train_ratio)\n",
    "    val_end = int(n_samples * (train_ratio + val_ratio))\n",
    "\n",
    "    X_train, Y_train = X_all[:train_end], Y_all[:train_end]\n",
    "    X_val, Y_val = X_all[train_end:val_end], Y_all[train_end:val_end]\n",
    "    X_test, Y_test = X_all[val_end:], Y_all[val_end:]\n",
    "\n",
    "    n_features = X_train.shape[2]\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    scaler_X.fit(X_train.reshape(-1, n_features))\n",
    "    X_train_scaled = scaler_X.transform(X_train.reshape(-1, n_features)).reshape(X_train.shape)\n",
    "    X_val_scaled = scaler_X.transform(X_val.reshape(-1, n_features)).reshape(X_val.shape)\n",
    "    X_test_scaled = scaler_X.transform(X_test.reshape(-1, n_features)).reshape(X_test.shape)\n",
    "\n",
    "    scaler_y.fit(Y_train.reshape(-1, 1))\n",
    "    Y_train_scaled = scaler_y.transform(Y_train.reshape(-1, 1)).ravel()\n",
    "    Y_val_scaled = scaler_y.transform(Y_val.reshape(-1, 1)).ravel()\n",
    "    Y_test_scaled = scaler_y.transform(Y_test.reshape(-1, 1)).ravel()\n",
    "\n",
    "    # 2) Model\n",
    "    input_shape = (lookback, n_features)\n",
    "    model = build_lstm_probabilistic(input_shape, units=units, dropout_rate=dropout_rate)\n",
    "    model.summary()\n",
    "\n",
    "    # For NLL training, we need two outputs per sample: mu and log_var.\n",
    "    # For a stable initialization, set log_var to small positive constant for targets in y-scale.\n",
    "    init_log_var = np.log(0.1)\n",
    "    y_train_targets = np.vstack([Y_train_scaled, np.ones_like(Y_train_scaled) * init_log_var]).T\n",
    "    y_val_targets = np.vstack([Y_val_scaled, np.ones_like(Y_val_scaled) * init_log_var]).T\n",
    "\n",
    "    # 3) Training with early stopping\n",
    "    early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True)\n",
    "    history = model.fit(\n",
    "        X_train_scaled,\n",
    "        y_train_targets,\n",
    "        validation_data=(X_val_scaled, y_val_targets),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early],\n",
    "        verbose=2,\n",
    "    )\n",
    "\n",
    "    # 4) Prediction with MC Dropout\n",
    "    mu_pred_scaled, epistemic_var_scaled, aleatoric_mean_scaled = mc_dropout_predict(\n",
    "        model, X_test_scaled, mc_samples=mc_samples\n",
    "    )\n",
    "\n",
    "    # Convert predicted means / variances back to original scale\n",
    "    mu_pred = scaler_y.inverse_transform(mu_pred_scaled.reshape(-1, 1)).ravel()\n",
    "    y_std = scaler_y.scale_[0]\n",
    "    aleatoric_mean = aleatoric_mean_scaled * (y_std ** 2)  # model predicted variance in scaled units -> original\n",
    "    epistemic_var = epistemic_var_scaled * (y_std ** 2)\n",
    "    total_var = aleatoric_mean + epistemic_var\n",
    "\n",
    "    # 5) Evaluate\n",
    "    metrics = evaluate_intervals(Y_test, mu_pred, total_var, ci_list=(0.8, 0.95))\n",
    "\n",
    "    # 6) Save artifacts and plots\n",
    "    os.makedirs(artifact_dir, exist_ok=True)\n",
    "    metrics_df = pd.DataFrame([metrics])\n",
    "    metrics_df.to_csv(os.path.join(artifact_dir, \"metrics.csv\"), index=False)\n",
    "\n",
    "    # Plot several segments from test set\n",
    "    n_plots = 6\n",
    "    step = max(1, len(Y_test) // n_plots)\n",
    "    plot_indices = list(range(0, len(Y_test), step))[:n_plots]\n",
    "    for idx in plot_indices:\n",
    "        start = max(0, idx - 60)\n",
    "        end = min(len(Y_test), idx + 60)\n",
    "        mu_seg = mu_pred[start:end]\n",
    "        y_seg = Y_test[start:end]\n",
    "        total_var_seg = total_var[start:end]\n",
    "        days_idx = np.arange(start, end)\n",
    "\n",
    "        z95 = 1.959963984540054\n",
    "        half_width95 = z95 * np.sqrt(total_var_seg)\n",
    "        lower95 = mu_seg - half_width95\n",
    "        upper95 = mu_seg + half_width95\n",
    "\n",
    "        z80 = 1.281551565545\n",
    "        half_width80 = z80 * np.sqrt(total_var_seg)\n",
    "        lower80 = mu_seg - half_width80\n",
    "        upper80 = mu_seg + half_width80\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(days_idx, y_seg, label=\"Actual\")\n",
    "        plt.plot(days_idx, mu_seg, label=\"Forecast (mean)\")\n",
    "        plt.fill_between(days_idx, lower95, upper95, alpha=0.2, label=\"95% PI\")\n",
    "        plt.fill_between(days_idx, lower80, upper80, alpha=0.4, label=\"80% PI\")\n",
    "        plt.legend()\n",
    "        plt.title(f\"Forecast vs Actual (indices {start}:{end})\")\n",
    "        fname = os.path.join(artifact_dir, f\"forecast_plot_{start}_{end}.png\")\n",
    "        plt.savefig(fname)\n",
    "        plt.close()\n",
    "\n",
    "    return {\"metrics\": metrics, \"artifact_dir\": artifact_dir, \"history\": history.history}\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# If running as script\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Quick-run configuration — adjust as needed for longer training/tuning\n",
    "    run_config = dict(\n",
    "        days=3 * 365,\n",
    "        lookback=30,\n",
    "        horizon=1,\n",
    "        train_ratio=0.7,\n",
    "        val_ratio=0.15,\n",
    "        epochs=15,\n",
    "        batch_size=32,\n",
    "        units=64,\n",
    "        dropout_rate=0.2,\n",
    "        mc_samples=100,\n",
    "        artifact_dir=\"ts_forecast_outputs\",\n",
    "    )\n",
    "\n",
    "    if not TF_AVAILABLE:\n",
    "        print(\"TensorFlow not installed in this environment. To run this script install TensorFlow:\")\n",
    "        print(\"  pip install 'tensorflow>=2.11'  # or use tensorflow-cpu if you prefer\")\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    results = run_pipeline(**run_config)\n",
    "    print(\"Results metrics:\")\n",
    "    for k, v in results[\"metrics\"].items():\n",
    "        print(f\"  {k}: {v:.6f}\")\n",
    "    print(\"Artifacts saved in:\", results[\"artifact_dir\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d8392f-c1ec-4565-85b5-0a3ca8741783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
